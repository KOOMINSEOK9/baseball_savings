{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜: 20250313, 팀: HH, 최대 페이지 수: 3\n",
      "크롤링 완료. 총 0개의 기사를 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행 (필요하면 제거)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver를 자동으로 다운로드 및 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 크롤링할 URL\n",
    "base_url = \"https://sports.news.naver.com/kbaseball/news/index\"\n",
    "\n",
    "# 최대 페이지 수 계산\n",
    "def get_max_page(date, team):\n",
    "    url = base_url + f\"?date={date}&team={team}&page=1&isphoto=N&type=team\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        pagination = driver.find_element(By.CLASS_NAME, \"paginate\")\n",
    "        pages = pagination.find_elements(By.TAG_NAME, \"a\")\n",
    "        return int(pages[-1].text) if pages else 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# 기사 목록 크롤링\n",
    "def crawl_articles(date, team, page):\n",
    "    articles = []\n",
    "    url = f\"{base_url}?date={date}&team={team}&page={page}&isphoto=N&type=team\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    # 뉴스 목록 가져오기\n",
    "    try:\n",
    "        news_list = driver.find_elements(By.CSS_SELECTOR, \".news_list > li\")\n",
    "        for item in news_list:\n",
    "            title_element = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            article_url = item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "            articles.append({\n",
    "                'news_title': title_element.text.strip(),\n",
    "                'article_url': article_url\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 본문 내용 추출\n",
    "def get_article_content(article_url):\n",
    "    driver.get(article_url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        news_content = driver.find_element(By.ID, \"newsEndContents\").text.strip()\n",
    "    except:\n",
    "        news_content = \"본문 없음\"\n",
    "\n",
    "    try:\n",
    "        published_date = driver.find_element(By.CSS_SELECTOR, \".info > span\").text.split(' ')[0]\n",
    "    except:\n",
    "        published_date = \"날짜 없음\"\n",
    "\n",
    "    return {\n",
    "        'news_content': news_content,\n",
    "        'published_date': published_date\n",
    "    }\n",
    "\n",
    "# 뉴스 기사 크롤링\n",
    "def crawl_news(date, team):\n",
    "    max_page = get_max_page(date, team)\n",
    "    print(f\"날짜: {date}, 팀: {team}, 최대 페이지 수: {max_page}\")\n",
    "    \n",
    "    all_articles = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        articles = crawl_articles(date, team, page)\n",
    "        for article in articles:\n",
    "            content = get_article_content(article['article_url'])\n",
    "            all_articles.append({\n",
    "                'news_title': article['news_title'],\n",
    "                'news_content': content['news_content'],\n",
    "                'published_date': content['published_date']\n",
    "            })\n",
    "        time.sleep(2)  # 요청 간 2초 대기\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "def main():\n",
    "    date = input(\"크롤링할 날짜를 입력하세요 (YYYYMMDD 형식): \")\n",
    "    # KIA 타이거즈: HT, 삼성 라이온즈: SS, 두산 베어스: OB, 롯데 자이언츠: LT, KT 위즈: KT, SSG 랜더스: SK, 한화 이글스: HH, NC 다이노스: NC, 키움 히어로즈: WO, LG 트윈스: LG\n",
    "    team = input(\"크롤링할 팀 코드를 입력하세요 (예: HT): \")\n",
    "    \n",
    "    results = crawl_news(date, team)\n",
    "    \n",
    "    with open(f\"news_{date}_{team}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"크롤링 완료. 총 {len(results)}개의 기사를 저장했습니다.\")\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행 (필요하면 제거)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver를 자동으로 다운로드 및 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 크롤링할 URL\n",
    "base_url = \"https://sports.news.naver.com/kbaseball/news/index\"\n",
    "\n",
    "# 최대 페이지 수 계산\n",
    "def get_max_page(date, team):\n",
    "    url = base_url + f\"?date={date}&team={team}&page=1&isphoto=N&type=team\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        pagination = driver.find_element(By.CLASS_NAME, \"paginate\")\n",
    "        pages = pagination.find_elements(By.TAG_NAME, \"a\")\n",
    "        return int(pages[-1].text) if pages else 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# 기사 목록 크롤링\n",
    "def crawl_articles(date, team):\n",
    "    max_page = get_max_page(date, team)\n",
    "    articles = []\n",
    "    \n",
    "    for page in range(1, max_page + 1):\n",
    "        url = f\"{base_url}?date={date}&team={team}&page={page}&isphoto=N&type=team\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "        # 뉴스 목록 가져오기\n",
    "        try:\n",
    "            news_list = driver.find_elements(By.CSS_SELECTOR, \"#_newsList > ul > li\")\n",
    "            for item in news_list:\n",
    "                title_element = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                article_url = item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "                articles.append({\n",
    "                    'news_title': re.sub(\"/\", \"\", title_element.text.strip()),\n",
    "                    'article_url': article_url\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return articles\n",
    "\n",
    "date = \"20250313\"\n",
    "team = \"HH\"\n",
    "max_page = get_max_page(date, team)\n",
    "print(max_page)\n",
    "\n",
    "crawl_articles(date, team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
