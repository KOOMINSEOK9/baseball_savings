{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구단별 뉴스기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행 (필요하면 제거)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver를 자동으로 다운로드 및 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 크롤링할 URL\n",
    "base_url = \"https://sports.news.naver.com/kbaseball/news/index\"\n",
    "\n",
    "# 최대 페이지 수 계산\n",
    "def get_max_page(date, team):\n",
    "    url = base_url + f\"?date={date}&team={team}&page=1&isphoto=N&type=team\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        pagination = driver.find_element(By.CLASS_NAME, \"paginate\")\n",
    "        pages = pagination.find_elements(By.TAG_NAME, \"a\")\n",
    "        return int(pages[-1].text) if pages else 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# 기사 목록 크롤링\n",
    "def crawl_articles(date, team, page):\n",
    "    articles = []\n",
    "    url = f\"{base_url}?date={date}&team={team}&page={page}&isphoto=N&type=team\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    # 뉴스 목록 가져오기\n",
    "    try:\n",
    "        news_list = driver.find_elements(By.CSS_SELECTOR, \".news_list > li\")\n",
    "        for item in news_list:\n",
    "            title_element = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "            article_url = item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "            articles.append({\n",
    "                'news_title': title_element.text.strip(),\n",
    "                'article_url': article_url\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 본문 내용 추출\n",
    "def get_article_content(article_url):\n",
    "    driver.get(article_url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        news_content = driver.find_element(By.ID, \"newsEndContents\").text.strip()\n",
    "    except:\n",
    "        news_content = \"본문 없음\"\n",
    "\n",
    "    try:\n",
    "        published_date = driver.find_element(By.CSS_SELECTOR, \".info > span\").text.split(' ')[0]\n",
    "    except:\n",
    "        published_date = \"날짜 없음\"\n",
    "\n",
    "    return {\n",
    "        'news_content': news_content,\n",
    "        'published_date': published_date\n",
    "    }\n",
    "\n",
    "# 뉴스 기사 크롤링\n",
    "def crawl_news(date, team):\n",
    "    max_page = get_max_page(date, team)\n",
    "    print(f\"날짜: {date}, 팀: {team}, 최대 페이지 수: {max_page}\")\n",
    "    \n",
    "    all_articles = []\n",
    "    for page in range(1, max_page + 1):\n",
    "        articles = crawl_articles(date, team, page)\n",
    "        for article in articles:\n",
    "            content = get_article_content(article['article_url'])\n",
    "            all_articles.append({\n",
    "                'news_title': article['news_title'],\n",
    "                'news_content': content['news_content'],\n",
    "                'published_date': content['published_date']\n",
    "            })\n",
    "        time.sleep(2)  # 요청 간 2초 대기\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "def main():\n",
    "    date = input(\"크롤링할 날짜를 입력하세요 (YYYYMMDD 형식): \")\n",
    "    # KIA 타이거즈: HT, 삼성 라이온즈: SS, 두산 베어스: OB, 롯데 자이언츠: LT, KT 위즈: KT, SSG 랜더스: SK, 한화 이글스: HH, NC 다이노스: NC, 키움 히어로즈: WO, LG 트윈스: LG\n",
    "    team = input(\"크롤링할 팀 코드를 입력하세요 (예: HT): \")\n",
    "    \n",
    "    results = crawl_news(date, team)\n",
    "    \n",
    "    with open(f\"news_{date}_{team}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"크롤링 완료. 총 {len(results)}개의 기사를 저장했습니다.\")\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날짜: 2025-03-13, 팀: KIA 타이거즈, 최대 페이지 수: 4\n",
      "CSV 파일 저장 완료: news_20250313_HT.csv\n",
      "크롤링 완료. 총 73개의 기사를 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행 (필요하면 제거)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver를 자동으로 다운로드 및 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 크롤링할 URL\n",
    "base_url = \"https://sports.news.naver.com/kbaseball/news/index\"\n",
    "\n",
    "# 최대 페이지 수 계산\n",
    "def get_max_page(date, team):\n",
    "    url = base_url + f\"?date={date}&team={team}&page=1&isphoto=N&type=team\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        pagination = driver.find_element(By.CLASS_NAME, \"paginate\")\n",
    "        pages = pagination.find_elements(By.TAG_NAME, \"a\")\n",
    "        return int(pages[-1].text) if pages else 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "# 기사 목록 크롤링\n",
    "def crawl_articles(date, team, max_page):\n",
    "    articles = []\n",
    "    \n",
    "    for page in range(1, max_page + 1):\n",
    "        url = base_url + f\"?date={date}&team={team}&page={page}&isphoto=N&type=team\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "        # 뉴스 목록 가져오기\n",
    "        try:\n",
    "            news_list = driver.find_elements(By.CSS_SELECTOR, \"#_newsList > ul > li\")\n",
    "            for item in news_list:\n",
    "                title_element = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                article_url = item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "                articles.append({\n",
    "                    'news_title': title_element.text.strip().replace(\"\\\\\", \"\"),\n",
    "                    'article_url': article_url\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 본문 내용 추출\n",
    "def get_article_content(article_url):\n",
    "    driver.get(article_url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        paragraphs = driver.find_elements(By.CSS_SELECTOR, \"span.article_p\")\n",
    "        news_content = \" \".join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "    except:\n",
    "        news_content = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        'news_content': news_content\n",
    "    }\n",
    "\n",
    "# 뉴스 기사 크롤링\n",
    "def crawl_news(date, team):\n",
    "    team_mapping = {\n",
    "    \"HT\": \"KIA 타이거즈\",\n",
    "    \"SS\": \"삼성 라이온즈\",\n",
    "    \"OB\": \"두산 베어스\",\n",
    "    \"LT\": \"롯데 자이언츠\",\n",
    "    \"KT\": \"KT 위즈\",\n",
    "    \"SK\": \"SSG 랜더스\",\n",
    "    \"HH\": \"한화 이글스\",\n",
    "    \"NC\": \"NC 다이노스\",\n",
    "    \"WO\": \"키움 히어로즈\",\n",
    "    \"LG\": \"LG 트윈스\"\n",
    "    }\n",
    "\n",
    "    published_date = date[:4] + \"-\" + date[4:6] + \"-\" + date[6:]\n",
    "    team_name = team_mapping.get(team, team)\n",
    "\n",
    "    max_page = get_max_page(date, team)\n",
    "    print(f\"날짜: {published_date}, 팀: {team_name}, 최대 페이지 수: {max_page}\")\n",
    "    \n",
    "    all_articles = []\n",
    "    articles = crawl_articles(date, team, max_page)\n",
    "    for article in articles:\n",
    "        content = get_article_content(article['article_url'])\n",
    "        all_articles.append({\n",
    "            'news_title': article['news_title'],\n",
    "            'news_content': content['news_content'],\n",
    "            'published_date': published_date\n",
    "        })\n",
    "    time.sleep(2)  # 요청 간 2초 대기\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "# CSV 저장 함수\n",
    "def save_to_csv(data, filename=\"news_results.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f\"news_csv/{filename}\", index=False, encoding=\"utf-8-sig\")  # 한글 깨짐 방지 위해 utf-8-sig 사용\n",
    "    print(f\"CSV 파일 저장 완료: {filename}\")\n",
    "\n",
    "def main():\n",
    "    date = input(\"크롤링할 날짜를 입력하세요 (YYYYMMDD 형식): \")\n",
    "    # KIA 타이거즈: HT, 삼성 라이온즈: SS, 두산 베어스: OB, 롯데 자이언츠: LT, KT 위즈: KT, SSG 랜더스: SK, 한화 이글스: HH, NC 다이노스: NC, 키움 히어로즈: WO, LG 트윈스: LG\n",
    "    team = input(\"크롤링할 팀 코드를 입력하세요 (예: HT): \")\n",
    "    \n",
    "    results = crawl_news(date, team)\n",
    "    \n",
    "    with open(f\"news_json/news_{date}_{team}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    save_to_csv(results, f\"news_{date}_{team}.csv\")\n",
    "    \n",
    "    print(f\"크롤링 완료. 총 {len(results)}개의 기사를 저장했습니다.\")\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OSEN=부산, 이상학 기자] “빠른 공에만 너무 포커스가 맞춰지는데…”\n",
      "13일 부산 사직구장에서 열린 프로야구 시범경기 롯데-한화전. 김경문 한화 감독은 지난 11일 문학 SSG전에서 최고 시속 159.7km 강속구를 뿌리며 큰 화제가 된 문동주에 대한 질문이 나오자 웃음꽃을 피우며 “(어깨 통증 이후 첫 등판이라) 기대하지 말라 그랬는데 좋았다. 팔 스윙이 작년 좋았을 때보다 좋게 나왔다. 내가 본 것 중에선 거의 베스트였다”고 칭찬했다.\n",
      "문동주에 이어 나온 김서현도 최고 시속 156km를 뿌리며 한화팬들을 밥 안 먹어도 배부르게 했다. 최고 154km까지 던진 신인 정우주까지 ‘젊은 파이어볼러 삼총사’가 구축된 한화는 외국인 투수 코디 폰세, 라이언 와이스 그리고 불펜 한승혁, 박상원 등 150km를 던지는 강속구 투수들이 넘친다.\n",
      "‘강속구 군단’ 한화에 관심이 높아지고 있는 가운데 김경문 감독은 “(관심이) 고맙기도 하고, 공 빠른 것이 자랑거리가 될 수 있지만 너무 거기에 포커스가 맞춰지지 않았으면 좋겠다. 야구는 공 빠른 것 외에 정교한 제구가 필요하다. (권)민규 같은 어린 친구도 볼이 빨라서 잘 던지는 게 아니다. 야구는 요소 요소에 밀고 넣는 강약 조절과 제구력이 굉장히 중요하다”고 말했다.\n",
      "가르친다고 되는 게 아닌 강속구는 축복받은 재능이고, 최근 몇 년 사이 고교 최고 강속구 투수들을 꾸준히 모은 한화는 타팀의 부러움을 한몸에 받고 있다. 하지만 김 감독은 강속구 풍요 속에 제구력의 중요성을 강조했고, 19세 신인 권민규를 예로 들었다. 권민규 이야기가 나올 때마다 김 감독은 “스카우들이 잘 뽑았다”고 말하는데 13일 롯데전에서 그 이유를 확인할 수 있었다.\n",
      "선발 류현진에 이어 5회 구원등판한 권민규는 첫 타자 전민재에게 초구 직구, 3구째 슬라이더를 모두 존에 넣어 투스트라이크를 잡았다. 4~5구 연속 파울이 됐지만 6구째 몸쪽 낮은 슬라이더로 헛스윙 삼진 처리했다.\n",
      "이어 좌타자 황성빈 상대로도 1~2구 연속 슬라이더, 직구를 바깥쪽에 던져 빠르게 투스트라이크를 점했다. 황성빈이 2개의 볼을 골라낸 뒤 연속 파울로 대응했지만 권민규는 7구째 바깥쪽 낮은 직구로 황성빈을 헛스윙 삼진 돌려세웠다. 보더라인에 살짝 벗어나는 스트라이크성 볼로 황성빈의 스윙을 이끌어냈다.\n",
      "두 타자만 상대하고 내려갔지만 권민규의 강점이 그대로 드러났다. 총 투구수 13개 중 스트라이크만 10개로 공격적이었다. 직구, 슬라이더 모두 스트라이크를 잡을 수 있는 제구가 있어 가능한 투구였다.\n",
      "세광고 출신으로 올해 2라운드 전체 12순위로 한화에 입단한 권민규는 지난해 고교 3학년 때 54⅓이닝 동안 볼넷 4개만 허용한 극강의 제구력을 뽐냈다. 강속구 투수 자원이 넘친 2025 신인 드래프트에서 2라운드로 밀려났지만 한화가 놓치지 않았다. 당초 2라운드에 야수 지명을 검토한 한화였지만 권민규가 남아있자 고민하지 않고 이름을 불렀다.\n",
      "즉시 전력감이 될 것으로 예상됐는데 기대보다 훨씬 빠른 속도로 프로 무대 적응 중이다. 호주 멜버른부터 일본 오키나와 스프링캠까지 이어진 대외 실전 4경기(1선발)에서 5⅔이닝 1피안타 1볼넷 7탈삼진 무실점으로 강한 인상을 남겼다. 여세를 몰아 시범경기에서도 3경기 2⅔이닝 1피안타 무사사구 5탈삼진 무실점 행진. 홀드도 2개를 기록했다. 캠프 연습경기 포함 대외 실전 7경기 8⅓이닝 무실점 행진. 안타 2개, 볼넷 1개만 주면서 삼진을 10개나 잡았다. 19세 고졸 신인답지 않은 안정감이다.\n",
      "‘구속 혁명’ 시대에 극강의 제구력으로 승부하는 권민규는 “고교 때 구속 욕심을 내다 팔이 아팠던 적이 있었는데 그 이후 무리해서 던지지 않는다. 제구는 원래 자신 있다. 구속보다 제구로 살아남겠다”고 말했다. 하지만 이날 롯데전에서 권민규의 직구 구속은 최고 시속 146km, 평균 143km로 충분히 경쟁력 있었다. 앞으로 힘이 더 붙으면 한화에 그야말로 ‘초대박’ 픽이 될 수 있다.\n",
      "/waw@osen.co.kr\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행 (필요하면 제거)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver를 자동으로 다운로드 및 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 크롤링할 URL\n",
    "article_url = 'https://m.sports.naver.com/kbaseball/article/109/0005260906'\n",
    "\n",
    "driver.get(article_url)\n",
    "time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "try:\n",
    "    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"span.article_p\")\n",
    "    news_content = \" \".join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "except:\n",
    "    news_content = \"본문 없음\"\n",
    "\n",
    "print(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경기별 뉴스기사 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    128\u001b[39m     driver.quit()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     date = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m크롤링할 날짜를 입력하세요 (YYYYMMDD 형식): \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# KIA 타이거즈: HT, 삼성 라이온즈: SS, 두산 베어스: OB, 롯데 자이언츠: LT, KT 위즈: KT, SSG 랜더스: SK, 한화 이글스: HH, NC 다이노스: NC, 키움 히어로즈: WO, LG 트윈스: LG\u001b[39;00m\n\u001b[32m    118\u001b[39m     team = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m크롤링할 팀 코드를 입력하세요 (예: HT): \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\Specialized_Project\\S12P21B206\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\Specialized_Project\\S12P21B206\\venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행 (필요하면 제거)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver를 자동으로 다운로드 및 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 크롤링할 URL\n",
    "base_url = \"https://m.sports.naver.com\"\n",
    "# https://m.sports.naver.com/kbaseball/schedule/index?date=2025-03-16\n",
    "# https://m.sports.naver.com/game/20250316HHNC02025/news\n",
    "\n",
    "# 일자별 경기 페이지 목록\n",
    "def get_game_list(date):\n",
    "    date_form = date[:4] + \"-\" + date[4:6] + \"-\" + date[6:]\n",
    "    url = base_url + f\"/kbaseball/schedule/index?date={date_form}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    games = driver.find_elements(By.CSS_SELECTOR, \"#content > div > div:nth-child(4) > div:nth-child(1) > ul > li > div > div.MatchBox_match_area__39dEr > a\")\n",
    "    return games\n",
    "\n",
    "# 기사 목록 크롤링\n",
    "def crawl_articles(date, team, max_page):\n",
    "    articles = []\n",
    "    \n",
    "    for page in range(1, max_page + 1):\n",
    "        url = base_url + f\"?date={date}&team={team}&page={page}&isphoto=N&type=team\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "        # 뉴스 목록 가져오기\n",
    "        try:\n",
    "            news_list = driver.find_elements(By.CSS_SELECTOR, \"#_newsList > ul > li\")\n",
    "            for item in news_list:\n",
    "                title_element = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                article_url = item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "                articles.append({\n",
    "                    'news_title': title_element.text.strip().replace(\"\\\\\", \"\"),\n",
    "                    'article_url': article_url\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 본문 내용 추출\n",
    "def get_article_content(article_url):\n",
    "    driver.get(article_url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    try:\n",
    "        paragraphs = driver.find_elements(By.CSS_SELECTOR, \"span.article_p\")\n",
    "        news_content = \" \".join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "    except:\n",
    "        news_content = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        'news_content': news_content\n",
    "    }\n",
    "\n",
    "# 뉴스 기사 크롤링\n",
    "def crawl_news(date, team):\n",
    "    team_mapping = {\n",
    "    \"HT\": \"KIA 타이거즈\",\n",
    "    \"SS\": \"삼성 라이온즈\",\n",
    "    \"OB\": \"두산 베어스\",\n",
    "    \"LT\": \"롯데 자이언츠\",\n",
    "    \"KT\": \"KT 위즈\",\n",
    "    \"SK\": \"SSG 랜더스\",\n",
    "    \"HH\": \"한화 이글스\",\n",
    "    \"NC\": \"NC 다이노스\",\n",
    "    \"WO\": \"키움 히어로즈\",\n",
    "    \"LG\": \"LG 트윈스\"\n",
    "    }\n",
    "\n",
    "    published_date = date[:4] + \"-\" + date[4:6] + \"-\" + date[6:]\n",
    "    team_name = team_mapping.get(team, team)\n",
    "\n",
    "    max_page = get_max_page(date, team)\n",
    "    print(f\"날짜: {published_date}, 팀: {team_name}, 최대 페이지 수: {max_page}\")\n",
    "    \n",
    "    all_articles = []\n",
    "    articles = crawl_articles(date, team, max_page)\n",
    "    for article in articles:\n",
    "        content = get_article_content(article['article_url'])\n",
    "        all_articles.append({\n",
    "            'news_title': article['news_title'],\n",
    "            'news_content': content['news_content'],\n",
    "            'published_date': published_date\n",
    "        })\n",
    "    time.sleep(2)  # 요청 간 2초 대기\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "# CSV 저장 함수\n",
    "def save_to_csv(data, filename=\"news_results.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f\"news_csv/{filename}\", index=False, encoding=\"utf-8-sig\")  # 한글 깨짐 방지 위해 utf-8-sig 사용\n",
    "    print(f\"CSV 파일 저장 완료: {filename}\")\n",
    "\n",
    "def main():\n",
    "    date = input(\"크롤링할 날짜를 입력하세요 (YYYYMMDD 형식): \")\n",
    "    # KIA 타이거즈: HT, 삼성 라이온즈: SS, 두산 베어스: OB, 롯데 자이언츠: LT, KT 위즈: KT, SSG 랜더스: SK, 한화 이글스: HH, NC 다이노스: NC, 키움 히어로즈: WO, LG 트윈스: LG\n",
    "    team = input(\"크롤링할 팀 코드를 입력하세요 (예: HT): \")\n",
    "    \n",
    "    results = crawl_news(date, team)\n",
    "    \n",
    "    with open(f\"news_json/news_{date}_{team}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    save_to_csv(results, f\"news_{date}_{team}.csv\")\n",
    "    \n",
    "    print(f\"크롤링 완료. 총 {len(results)}개의 기사를 저장했습니다.\")\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://m.sports.naver.com/game/20240731HHKT02024', 'https://m.sports.naver.com/game/20240731LTSK02024', 'https://m.sports.naver.com/game/20240731NCWO02024', 'https://m.sports.naver.com/game/20240731OBHT02024', 'https://m.sports.naver.com/game/20240731SSLG02024']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 크롬 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # GUI 없이 실행 (필요하면 제거)\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver를 자동으로 다운로드 및 설정\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# 크롤링할 URL\n",
    "base_url = \"https://m.sports.naver.com\"\n",
    "\n",
    "# 일자별 경기 페이지 목록\n",
    "def get_game_list(date):\n",
    "    date_form = date[:4] + \"-\" + date[4:6] + \"-\" + date[6:]\n",
    "    url = base_url + f\"/kbaseball/schedule/index?date={date_form}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    games = driver.find_elements(By.CSS_SELECTOR, \"#content > div > div:nth-child(4) > div:nth-child(1) > ul > li > div > div.MatchBox_match_area__39dEr > a\")\n",
    "\n",
    "    game_list = []\n",
    "    for game in games:\n",
    "        game_url = game.get_attribute(\"href\")\n",
    "        home_team = game.find_element(By.CSS_SELECTOR, \"div.MatchBox_team__1YR1v:nth-child(1) > div.MatchBox_team_name__2k3YB\").text\n",
    "        away_team = game.find_element(By.CSS_SELECTOR, \"div.MatchBox_team__1YR1v:nth-child(2) > div.MatchBox_team_name__2k3YB\").text\n",
    "        game_list.append({\n",
    "            'game_url': game_url,\n",
    "            'home_team': home_team,\n",
    "            'away_team': away_team\n",
    "        })\n",
    "    return game_list\n",
    "\n",
    "# 기사 목록 크롤링\n",
    "def crawl_articles(date, team):\n",
    "    articles = []\n",
    "    \n",
    "    for page in range(1, max_page + 1):\n",
    "        url = base_url + f\"?date={date}&team={team}&page={page}&isphoto=N&type=team\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "        # 뉴스 목록 가져오기\n",
    "        try:\n",
    "            news_list = driver.find_elements(By.CSS_SELECTOR, \"#_newsList > ul > li\")\n",
    "            for item in news_list:\n",
    "                title_element = item.find_element(By.CSS_SELECTOR, \".title\")\n",
    "                article_url = item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "                articles.append({\n",
    "                    'news_title': title_element.text.strip().replace(\"\\\\\", \"\"),\n",
    "                    'article_url': article_url\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return articles\n",
    "\n",
    "date=\"20240731\"\n",
    "game_list=get_game_list(date)\n",
    "print(game_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
